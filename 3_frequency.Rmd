---
title: "3. Частотность слов"
date: "26 июля 2017, АНДАН"
author: "Г. Мороз"
---
<style>
.parallax {
    /* The image used */
    background-image: url("3_frequency.jpg");

    /* Set a specific height */
    min-height: 360px; 

    /* Create the parallax scrolling effect */
    background-attachment: fixed;
    background-position: center;
    background-repeat: no-repeat;
    background-size: length;
}
</style>

>  "Интересно, кто же это такой Слонопотам?"-- подумал Пух. <br> -- Их не часто встретишь,-- небрежно сказал Кристофер Робин. <div style="float: right; clear: right; margin: 13px;">(А. А. Милн)</div>

<div class="parallax"></div>

### 1. Подготовка[^1]

[^1]: Я использовал один из рисунков Эрнеста Говарда Шепарда к Винни-Пуху.

Начнем с того, что скачаем датасет, с которым мы будем работать.
```{r}
temp <- tempfile() # создаем временный файл
path <- "./docs/materials/Chekhov/"
download.file("https://goo.gl/9DWBF5", destfile = temp) # скачиваем в него архив
unzip(temp, exdir = path) # создаем папку Chekhov и распаковываем туда
rm(temp) # удаляем временный файл
list.files(path = path) # смотрим на список распокованных файлов
```

Это 30 маленьких рассказов А. П. Чехова. Давайте все их считаем:
```{r}
files <- list.files(path = path) # создадим переменную со списком файлов
texts <- lapply(paste0(path, files), FUN=readLines) # считаем все файлы в одну переменную
```

<div class="parallax"></div>

### 2. Введение в `tidytext`
Обычная философия **tidy data**: одна строчка -- одно наблюдение; один столбец -- одна переменная. Попробуем tidyфицировать один текст:
```{r, message = FALSE}
library(tidyverse); library(tidytext); library(stringr)
text_df <- data_frame(line = seq_along(texts[[1]]),
                              text = texts[[1]])
head(text_df)
text_df %>%
  unnest_tokens(word, text) ->
  tidy_df
head(tidy_df)
```

Чтобы избежать изменения размера шрифта нужно использовать аргумент `to_lower = FALSE`
```{r}
text_df %>%
  unnest_tokens(word, text, to_lower = FALSE) ->
  tidy_df
head(tidy_df)
```

На следующем шаге хотелось бы создать датафрейм со всем произведениями, которые мы рассматриваем.

```{r}
texts <- lapply(seq_along(texts), function(x){
  data_frame(title = files[[x]],
             sentences = seq_along(texts[[x]]),
             text = texts[[x]])
})

all_texts <- Reduce(function(x,y){merge(x,y, all = TRUE)}, texts)

all_texts %>%
  unnest_tokens(word, text) ->
  tidy_chekhov
head(tidy_chekhov)
```


<div class="parallax"></div>

### 3. Закон Хердана-Хипса
Закон Хердана-Хипса (Herdan-Heaps' law) -- имперический закон, согласно которому количество уникальных слов в тексте зависит от длины текста.

$$V(n) = K\times n ^{β}$$

* V(n) -- количество уникальных слов в тексте длины n
* обычно K между 10 и 100
* обычно β между 0.4 и 0.6

```{r}
tidy_chekhov %>% 
  group_by(title) %>% 
  summarise(n_words = n(),
            n_unique = length(unique(word))) ->
  heaps
heaps  %>% 
  ggplot(aes(n_words, n_unique))+
  geom_point()+
  theme_bw()+
  labs(title = "Иллюстрация закона Хердана-Хипса на примере рассказов Чехова",
       x = "количество слов",
       y = "количество уникальных слов")
```

```{r}
fit <- summary(lm(n_unique~sqrt(n_words)-1, data = heaps))
fit$coefficients
fit$adj.r.squared
```

<div class="parallax"></div>

### 4. Закон Ципфа
Закон Ципфа (Zipf's law) -- имперический закон, согласно которому частотность слова обратно пропорционально его рангу.

$$freq(r) = A \times N \times r^{-1}$$

* freq(r) -- частотность слова с рангом r
* N -- общее количество слов
* A -- обычно 0.1

```{r}
tidy_chekhov %>% 
  group_by(word) %>% 
  summarise(term_frequency = n()/nrow(tidy_chekhov)) %>% 
  arrange(desc(term_frequency)) %>% 
  mutate(rank = row_number()) -> zipf
head(zipf)
```

```{r}
zipf %>% 
  ggplot(aes(rank, term_frequency))+
  geom_line()+
  scale_x_log10() +
  scale_y_log10() +
  labs(title = "Иллюстрация закона Хердана-Хипса на примере рассказов Чехова",
       y = "log(term frequency)",
       x= "log(rank)")+
  theme_bw()
```


<div class="parallax"></div>

### 5. Стоп слова
Чтобы начать анализировать текст нужно удалить "служебные" слова, которые часто встречаются в тексте, но не очень приближают к его пониманию. Такие списки для английского языка встроены в пакет `tidytext` (см. `data(stop_words)`). Для русского языка, конечно, ничего не встроено, но подобные списки легко гуглятся (я буду использовать  [этот](https://github.com/stopwords-iso/stopwords-ru)).

> ОБЯЗАТЕЛЬНО: проверьте используется ли буква _ё_ в ваших данных и в вашем списке стоп слов.

```{r}
str_detect(tidy_chekhov, "ё")
```
```{r}
ru_stop_words <- read.csv("https://goo.gl/pfpUrB", header = FALSE)
str_detect(ru_stop_words, "ё")
```

```{r}
tidy_chekhov$word <- str_replace_all(tidy_chekhov$word, "ё", "е")
tidy_chekhov %>% 
  filter(!word %in% ru_stop_words$V1) %>%
  filter(str_detect(title, "moi_jeni.txt|nalim.txt|jeger.txt")) %>%
  count(title, word, sort = TRUE) %>% 
  filter(n > 3) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n))+
  coord_flip()+
  geom_bar(stat= "identity")+
  facet_wrap(~title, scale = "free")+
  theme_bw()
```

<div class="parallax"></div>

### 6. TfIdf
Откидывание стоп-слов позволяет не анализировать малозначимые слова. Однако как показать, что какие-то два текста похожи, а другие различаются? Для этого используется мера TfIdf (term frequency inverse document frequency).

$$tf = \frac{количество\ употреблений\ единицы\ в\ тексте}{количество\ уникальных\ единиц\ в тексте}$$
$$idf = log\left(\frac{количество\ документов\ в\ корпусе}{количество\ документов\ с\ исследуемой\ единицей}\right)$$
$$TfIdf = tf*idf$$

```{r}
tidy_chekhov %>% 
  count(title, word)%>% 
  arrange(desc(n)) %>% 
  bind_tf_idf(word, title, n) ->
  tidy_chekhov_tf_idf
head(tidy_chekhov_tf_idf)

tidy_chekhov_tf_idf %>% 
  filter(tf_idf > 0.03) %>% # число подобрано по данным
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill = title))+
  coord_flip()+
  geom_bar(stat= "identity")+
  theme_bw()
```

<div class="parallax"></div>

### 7. Частота как переменная
```{r}
library(ggfortify)
gospels <- read.csv("https://goo.gl/mdBVVe")
head(gospels)
row.names(gospels) <- gospels$word
PCA <- prcomp(gospels[,2:5])
autoplot(PCA,
         shape = FALSE,
         loadings = TRUE,
         label = TRUE,
         loadings.label = TRUE)+
  theme_bw()
summary(PCA)
```

### 8. n-граммы
Все это время мы работали с униграммами, однако достаточно много информации дает биграмное, триграммное и т. д. представление. Для того, чтобы получить n-граммы нужно использовать уже известную функцию `unnest_tokens()` с аргументом `token = "ngrams"`

```{r}
all_texts %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  mutate(bigram = str_replace_all(bigram, "ё", "е")) ->
  tidy_chekhov_bigrams

head(tidy_chekhov_bigrams)
```

Биграмма -- это строка... Надо распилить

```{r}
tidy_chekhov_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") ->
  tidy_chekhov_bigrams
head(tidy_chekhov_bigrams)
```

Выкинем стоп-слова:

```{r}
tidy_chekhov_bigrams %>% 
  filter(!word1 %in% ru_stop_words$V1) %>%
  filter(!word2 %in% ru_stop_words$V1) ->
  tidy_chekhov_bigrams
head(tidy_chekhov_bigrams)
```

```{r}
tidy_chekhov_bigrams %>% 
  group_by(title) %>% 
  count(word1, word2, sort = TRUE) ->
  tidy_chekhov_bigrams
head(tidy_chekhov_bigrams)
```

Разъединили? Можно и объединитьǃ

```{r}
tidy_chekhov_bigrams %>% 
  unite(bigram, word1, word2, sep = " ") ->
  tidy_chekhov_bigrams
head(tidy_chekhov_bigrams)

tidy_chekhov_bigrams %>% 
  filter(n > 3) %>% # число опять от фонаря
  ggplot(aes(bigram, n, fill = title))+
  geom_bar(stat = "identity")+
  coord_flip()+
  theme_bw()
```

Можно опять же посчитать TfIdf:

```{r}
tidy_chekhov_bigrams %>% 
  bind_tf_idf(bigram, title, n) %>%
  arrange(desc(tf_idf)) -> 
  tidy_chekhov_bigrams_tf_idf
head(tidy_chekhov_bigrams_tf_idf)
```

```{r}
tidy_chekhov_bigrams_tf_idf %>% 
  filter(tf_idf > 0.04) %>% # число подобрано по данным
  ggplot(aes(bigram, tf_idf, fill = title))+
  coord_flip()+
  geom_bar(stat= "identity")+
  theme_bw()
```


### 9. Задачи
#### 9.1 Еще немножко о законе Ципфа
У функции `unnest_tokens()` есть аргумент `token`, значение которого по умолчанию `"words"`. Попробуйте использовать значение `"characters"` и определить будет ли соблюдаться закон Ципфа для отдельных символов.
```{r, include=FALSE}
all_texts %>%
  unnest_tokens(characters, text, token = "characters") %>% 
  group_by(characters) %>% 
  summarise(term_frequency = n()/nrow(tidy_chekhov)) %>% 
  arrange(desc(term_frequency)) %>% 
  mutate(rank = row_number()) %>% 
    ggplot(aes(rank, term_frequency))+
  geom_line()+
  scale_x_log10() +
  scale_y_log10() +
  labs(title = "Иллюстрация закона Хердана-Хипса на примере рассказов Чехова",
       y = "log(term frequency)",
       x= "log(rank)")+
  theme_bw()
```

<div class="parallax"></div>